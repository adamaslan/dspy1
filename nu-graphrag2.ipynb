{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Knowledge graph loaded successfully.\n",
      "Query: How can mindfulness improve well-being and inner beauty?\n",
      "Debug: Searching for relevant nodes and edges for query: How can mindfulness improve well-being and inner beauty?\n",
      "Debug: Found relevant node: mindfulness\n",
      "Debug: Found relevant edge: ('inner_beauty', 'mindfulness', {'relationship': 'promotes'})\n",
      "An error occurred: unhashable type: 'dict'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize GPT-2 Model\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize the tokenizer and GPT-2 model.\"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")  # Set padding_side to 'left'\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").half()  # Load GPT-2 in half-precision\n",
    "    model.to(device)  # Ensure itâ€™s on the correct device\n",
    "\n",
    "    # Set pad_token (GPT-2 doesn't have one by default, so we use eos_token as pad_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Create Knowledge Graph from CSV\n",
    "def create_knowledge_graph_from_csv(csv_path):\n",
    "    \"\"\"Create a knowledge graph from a CSV file.\"\"\"\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Read CSV file\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        idea = row['Ideas']\n",
    "        themes = [row['Theme a'], row['Theme-b'], row['Theme-c']]\n",
    "\n",
    "        # Add nodes for the idea and its themes\n",
    "        graph.add_node(idea)\n",
    "        for theme in themes:\n",
    "            if pd.notna(theme):\n",
    "                graph.add_node(theme)\n",
    "                graph.add_edge(idea, theme, relationship=\"relates_to\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Retrieve Knowledge from Graph\n",
    "def retrieve_from_graph(graph, query):\n",
    "    \"\"\"Retrieve relevant information from the knowledge graph based on the input query.\"\"\"\n",
    "    relevant_nodes = set()\n",
    "    relevant_edges = set()\n",
    "\n",
    "    print(f\"Debug: Searching for relevant nodes and edges for query: {query}\")\n",
    "    for node in graph.nodes:\n",
    "        if any(keyword.lower() in query.lower() for keyword in node.split()):\n",
    "            print(f\"Debug: Found relevant node: {node}\")\n",
    "            relevant_nodes.add(node)\n",
    "\n",
    "    for edge in graph.edges(data=True):\n",
    "        if edge[0] in relevant_nodes or edge[1] in relevant_nodes:\n",
    "            print(f\"Debug: Found relevant edge: {edge}\")\n",
    "            relevant_edges.add(edge)\n",
    "\n",
    "    return list(relevant_nodes), list(relevant_edges)\n",
    "\n",
    "# Generate Context-Aware Responses\n",
    "def generate_with_graph_knowledge(query, model, tokenizer, graph):\n",
    "    \"\"\"Generate a response based on a query and relevant knowledge from the graph.\"\"\"\n",
    "    relevant_nodes, relevant_edges = retrieve_from_graph(graph, query)\n",
    "\n",
    "    # If no knowledge is found, provide a fallback message\n",
    "    if not relevant_nodes and not relevant_edges:\n",
    "        context = f\"Query: {query}\\nRelevant Knowledge: No relevant knowledge found in the graph.\"\n",
    "    else:\n",
    "        context = f\"Query: {query}\\nRelevant Knowledge:\\n\"\n",
    "        for node in relevant_nodes:\n",
    "            context += f\" - Node: {node}\\n\"\n",
    "        for edge in relevant_edges:\n",
    "            context += f\" - Edge: {edge[0]} ({edge[2]['relationship']}) {edge[1]}\\n\"\n",
    "\n",
    "    # Use GPT-2 to generate a response\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=200, \n",
    "        num_return_sequences=1, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Save and Load Knowledge Graph\n",
    "def save_graph(graph, filepath=\"knowledge_graph.pkl\"):\n",
    "    \"\"\"Save the knowledge graph to a file.\"\"\"\n",
    "    import pickle\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "def load_graph(filepath=\"knowledge_graph.pkl\"):\n",
    "    \"\"\"Load the knowledge graph from a file.\"\"\"\n",
    "    import pickle\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Main Processing Function\n",
    "def main():\n",
    "    \"\"\"Main processing loop.\"\"\"\n",
    "    # Filepath to save/load the graph\n",
    "    save_path = \"knowledge_graph.pkl\"\n",
    "    csv_path = \"151_ideas_updated.csv\"\n",
    "\n",
    "    # Load or create the knowledge graph\n",
    "    try:\n",
    "        graph = load_graph(save_path)\n",
    "        print(\"Knowledge graph loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        graph = create_knowledge_graph_from_csv(csv_path)\n",
    "        save_graph(graph, save_path)\n",
    "        print(\"Knowledge graph created and saved.\")\n",
    "\n",
    "    # Initialize the model\n",
    "    tokenizer, model = initialize_model()\n",
    "\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"How can mindfulness improve well-being and inner beauty?\",\n",
    "        \"What is the relationship between pragmatism and philosophy?\",\n",
    "        \"Explain the connection between meditation and personal growth.\",\n",
    "        \"How does Nietzsche's view on intention relate to modern life?\",\n",
    "        \"What role does beauty play in rational decision-making?\"\n",
    "    ]\n",
    "\n",
    "    # Generate responses for each query\n",
    "    for query in queries:\n",
    "        print(f\"Query: {query}\")\n",
    "        response = generate_with_graph_knowledge(query, model, tokenizer, graph)\n",
    "        print(f\"Response:\\n{response}\\n\")\n",
    "\n",
    "# Example Execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()  # Process the queries and generate responses\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
