{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using device: cpu\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using device: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cl2\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Debugging: Print Python and Library Versions\n",
    "import sys\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"DSPy Version:\", dspy.__version__)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    # Load CSV with extra diagnostics\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    print(\"\\nCSV Load Successful\")\n",
    "    print(\"CSV Columns:\", list(data.columns))\n",
    "    print(\"Total Rows:\", len(data))\n",
    "    print(\"\\nFirst few rows:\\n\", data.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CSV Loading Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Model and Tokenizer Loading with Extensive Debugging\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "try:\n",
    "    print(\"\\nLoading Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    print(\"Tokenizer Loaded Successfully\")\n",
    "\n",
    "    print(\"\\nLoading Model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model, \n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    ).to(device)\n",
    "    print(\"Model Loaded Successfully\")\n",
    "\n",
    "    print(\"\\nCreating Pipeline...\")\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "    print(\"Pipeline Created Successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model Loading Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Basic Pipeline Test\n",
    "try:\n",
    "    print(\"\\nTesting Text Generation...\")\n",
    "    test_prompt = \"Hello, how are you?\"\n",
    "    result = text_generator(test_prompt, max_length=50)\n",
    "    print(\"Generation Test Result:\")\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Generation Test Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using device: cpu\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using device: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CSV Data Columns:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 5'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CSV Data Columns:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Ideas'\u001b[0m, \u001b[32m'Theme a'\u001b[0m, \u001b[32m'Theme-b'\u001b[0m, \u001b[32m'Theme-c'\u001b[0m, \u001b[32m'Unnamed: 4'\u001b[0m, \u001b[32m'Unnamed: 5'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">First few rows:\n",
       "                                                Ideas    Theme a    Theme-b  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span> Maximize the Beauty - fully channel the bea<span style=\"color: #808000; text-decoration-color: #808000\">...</span>        fun   rational   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> Full Expression - it takes a lot of effort <span style=\"color: #808000; text-decoration-color: #808000\">...</span>  inspiring  intuitive   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> - Neech real/dumb - Nietzsche had some grea<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   negative      spicy   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span> - Camus - beautiful not pragmatic - I love <span style=\"color: #808000; text-decoration-color: #808000\">...</span>    opinion   rational   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span> games – Be a good lil sociopath path and s<span style=\"color: #808000; text-decoration-color: #808000\">...</span>      rough      spicy   \n",
       "\n",
       "    Theme-c Unnamed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                                         Unnamed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  positive   personal  theme ideas - rough, rational, intuitive, posi<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  positive   personal  top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> idea themes- rational, positive, inspiri<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>     neech    opinion                                              phil   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>   neutral   opinion                                               phil   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>   emotion   rational                                          inspiring  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "First few rows:\n",
       "                                                Ideas    Theme a    Theme-b  \\\n",
       "\u001b[1;36m0\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m Maximize the Beauty - fully channel the bea\u001b[33m...\u001b[0m        fun   rational   \n",
       "\u001b[1;36m1\u001b[0m  \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m Full Expression - it takes a lot of effort \u001b[33m...\u001b[0m  inspiring  intuitive   \n",
       "\u001b[1;36m2\u001b[0m  \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m - Neech real/dumb - Nietzsche had some grea\u001b[33m...\u001b[0m   negative      spicy   \n",
       "\u001b[1;36m3\u001b[0m  \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m - Camus - beautiful not pragmatic - I love \u001b[33m...\u001b[0m    opinion   rational   \n",
       "\u001b[1;36m4\u001b[0m  \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m games – Be a good lil sociopath path and s\u001b[33m...\u001b[0m      rough      spicy   \n",
       "\n",
       "    Theme-c Unnamed: \u001b[1;36m4\u001b[0m                                         Unnamed: \u001b[1;36m5\u001b[0m  \n",
       "\u001b[1;36m0\u001b[0m  positive   personal  theme ideas - rough, rational, intuitive, posi\u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m  positive   personal  top \u001b[1;36m5\u001b[0m idea themes- rational, positive, inspiri\u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m     neech    opinion                                              phil   \n",
       "\u001b[1;36m3\u001b[0m   neutral   opinion                                               phil   \n",
       "\u001b[1;36m4\u001b[0m   emotion   rational                                          inspiring  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DSPy Chatbot: Hello! I'm ready to help. Type <span style=\"color: #008000; text-decoration-color: #008000\">'exit'</span> to quit.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DSPy Chatbot: Hello! I'm ready to help. Type \u001b[32m'exit'\u001b[0m to quit.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded dataset columns:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 5'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded dataset columns:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Ideas'\u001b[0m, \u001b[32m'Theme a'\u001b[0m, \u001b[32m'Theme-b'\u001b[0m, \u001b[32m'Theme-c'\u001b[0m, \u001b[32m'Unnamed: 4'\u001b[0m, \u001b[32m'Unnamed: 5'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cl kinda works\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "import torch\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "\n",
    "    # Clean and prepare the data\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    print(\"CSV Data Columns:\", list(data.columns))\n",
    "    print(\"First few rows:\\n\", data.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load Hugging Face model and tokenizer\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model, \n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    # Create text generation pipeline with explicit device\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if device == \"cuda\" else -1  # use GPU if available\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Custom Language Model for DSPy\n",
    "class SimpleLLM(dspy.LM):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.generator(\n",
    "                prompt, \n",
    "                max_length=100, \n",
    "                num_return_sequences=1\n",
    "            )[0]['generated_text']\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"I'm having trouble generating a response.\"\n",
    "\n",
    "# Define a simple signature for the chatbot\n",
    "class ChatbotSignature(dspy.Signature):\n",
    "    \"\"\"Generate a helpful and concise response to a user's query.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    response = dspy.OutputField(desc=\"Helpful and relevant answer\")\n",
    "\n",
    "# Configure DSPy with the custom language model\n",
    "dspy.settings.configure(lm=SimpleLLM(text_generator))\n",
    "\n",
    "# Create a prediction module\n",
    "chatbot = dspy.Predict(ChatbotSignature)\n",
    "\n",
    "# Interactive chat loop\n",
    "def chat():\n",
    "    print(\"DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\")\n",
    "    print(\"Loaded dataset columns:\", list(data.columns))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            # Exit condition\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"DSPy Chatbot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Enhance prompt with context from data if possible\n",
    "            # Assumes there's a column that might be relevant\n",
    "            context_column = data.columns[0] if len(data.columns) > 0 else None\n",
    "            \n",
    "            if context_column:\n",
    "                # Add some context from the dataset\n",
    "                enhanced_prompt = f\"Using information from the dataset, {user_input}\"\n",
    "            else:\n",
    "                enhanced_prompt = user_input\n",
    "            \n",
    "            # Generate response\n",
    "            response = chatbot(query=enhanced_prompt)\n",
    "            print(\"DSPy Chatbot:\", response.response)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {e}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\n",
      "Dataset columns: ['Ideas', 'Theme a', 'Theme-b', 'Theme-c', 'Unnamed: 4', 'Unnamed: 5']\n"
     ]
    }
   ],
   "source": [
    "# m2 claude optimized \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Optimized for M2 Mac\n",
    "huggingface_model = 'distilgpt2'  # Smaller, more lightweight model\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "\n",
    "# Clean the data\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# Load tokenizer and model with MPS (Metal Performance Shaders) support for M2\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    \n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Use MPS for M2 Mac\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Create text generation pipeline\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model loading error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Custom Language Model for DSPy\n",
    "class SimpleLLM(dspy.LM):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        try:\n",
    "            # Generate response\n",
    "            responses = self.generator(\n",
    "                prompt, \n",
    "                max_length=100, \n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return responses[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"I'm having trouble generating a response.\"\n",
    "\n",
    "# Define chatbot signature\n",
    "class ChatbotSignature(dspy.Signature):\n",
    "    \"\"\"Generate a helpful and concise response to a user's query.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    response = dspy.OutputField(desc=\"Helpful and relevant answer\")\n",
    "\n",
    "# Configure DSPy\n",
    "dspy.settings.configure(lm=SimpleLLM(text_generator))\n",
    "\n",
    "# Create prediction module\n",
    "chatbot = dspy.Predict(ChatbotSignature)\n",
    "\n",
    "# Interactive chat loop\n",
    "def chat():\n",
    "    print(\"DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\")\n",
    "    print(\"Dataset columns:\", list(data.columns))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"DSPy Chatbot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Add context from the dataset\n",
    "            context = data['Ideas'].sample(1).values[0]\n",
    "            enhanced_prompt = f\"Context from dataset: {context}\\n\\nUser query: {user_input}\"\n",
    "            \n",
    "       \n",
    "       \n",
    "            # Generate response\n",
    "            response = chatbot(query=enhanced_prompt)\n",
    "            print(\"DSPy Chatbot:\", response.response)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {e}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 119, Devset size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 4.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(huggingface_model)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a text generation pipeline\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Improved Custom Language Model using DSPy LM\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHuggingFaceLM\u001b[39;00m(dspy\u001b[38;5;241m.\u001b[39mLM):\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:926\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    925\u001b[0m ):\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 4.00 KB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# claude - 12-16-24\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "\n",
    "try:\n",
    "    # Load and preprocess the dataset\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')  # Drop unnecessary columns\n",
    "    data = data.dropna().reset_index(drop=True)  # Remove NaN values and reset index\n",
    "\n",
    "    # Convert to Hugging Face Dataset and split into train and test sets\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    trainset, devset = dataset['train'], dataset['test']\n",
    "\n",
    "    print(f\"Trainset size: {len(trainset)}, Devset size: {len(devset)}\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "\n",
    "# Load Hugging Face tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Improved Custom Language Model using DSPy LM\n",
    "class HuggingFaceLM(dspy.LM):\n",
    "    def __init__(self, generator, temperature=0.7, max_length=50):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        # Ensure method is compatible with DSPy's expectation\n",
    "        return self.generate(prompt, **kwargs)\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generate output text given a prompt.\"\"\"\n",
    "        try:\n",
    "            # Use provided kwargs or fall back to instance defaults\n",
    "            temperature = kwargs.get('temperature', self.temperature)\n",
    "            max_length = kwargs.get('max_length', self.max_length)\n",
    "            \n",
    "            output = self.generator(\n",
    "                prompt, \n",
    "                max_length=max_length, \n",
    "                temperature=temperature,\n",
    "                do_sample=True  # Enable sampling for temperature effect\n",
    "            )\n",
    "            return output[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def score(self, completions, **kwargs):\n",
    "        # Placeholder scoring method\n",
    "        return [0.0] * len(completions)\n",
    "\n",
    "# Configure DSPy Language Model\n",
    "lm = HuggingFaceLM(text_generator)\n",
    "\n",
    "# Optimized DSPy Configuration\n",
    "dspy.settings.configure(\n",
    "    lm=lm,\n",
    "    rm=colbertv2_wiki17_abstracts,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the Idea Summarization Signature\n",
    "class IdeaSummarizer(dspy.Signature):\n",
    "    \"\"\"Summarize ideas into concise, theme-aligned outputs.\"\"\"\n",
    "    idea = dspy.InputField(desc=\"A creative or philosophical idea to summarize.\")\n",
    "    themes = dspy.InputField(desc=\"Themes associated with this idea.\")\n",
    "    summary = dspy.OutputField(desc=\"A concise 1-5 word summary of the idea.\")\n",
    "\n",
    "# Define the Basic Question-Answering Signature\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "    question = dspy.InputField(desc=\"A question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"A concise answer between 1 and 5 words\")\n",
    "\n",
    "# Example pipeline for summarizing ideas\n",
    "example = devset[0]\n",
    "idea = example['Ideas']\n",
    "themes = \", \".join([example.get('Theme a', ''), example.get('Theme-b', ''), example.get('Theme-c', '')]).strip(\", \")\n",
    "question = example['Ideas']\n",
    "\n",
    "# Create prediction pipelines\n",
    "generate_summary = dspy.Predict(IdeaSummarizer)\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Generate summary\n",
    "try:\n",
    "    result_summary = generate_summary(idea=idea, themes=themes)\n",
    "    print(\"Idea Summary:\")\n",
    "    print(f\"Idea: {idea}\")\n",
    "    print(f\"Themes: {themes}\")\n",
    "    print(f\"Summary: {result_summary.summary}\")\n",
    "    \n",
    "    # Generate answer\n",
    "    result_answer = generate_answer(question=question)\n",
    "    print(\"\\nQuestion Answer:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result_answer.answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DSPy pipeline execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 119, Devset size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 98.19 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(huggingface_model)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create a text generation pipeline\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Improved Custom Language Model using DSPy LM\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHuggingFaceLM\u001b[39;00m(dspy\u001b[38;5;241m.\u001b[39mLM):\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:926\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    925\u001b[0m ):\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 98.19 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "\n",
    "try:\n",
    "    # Load and preprocess the dataset\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')  # Drop unnecessary columns\n",
    "    data = data.dropna().reset_index(drop=True)  # Remove NaN values and reset index\n",
    "\n",
    "    # Convert to Hugging Face Dataset and split into train and test sets\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    trainset, devset = dataset['train'], dataset['test']\n",
    "\n",
    "    print(f\"Trainset size: {len(trainset)}, Devset size: {len(devset)}\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "\n",
    "# Load Hugging Face tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Improved Custom Language Model using DSPy LM\n",
    "class HuggingFaceLM(dspy.LM):\n",
    "    def __init__(self, generator, model=None, temperature=0.7, max_length=50):\n",
    "        model = model or generator.model\n",
    "        super().__init__(model=model)\n",
    "        self.generator = generator\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            raise ValueError(\"Prompt must be a non-empty string.\")\n",
    "        return self.generate(prompt, **kwargs)\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        try:\n",
    "            temperature = kwargs.get('temperature', self.temperature)\n",
    "            max_length = kwargs.get('max_length', self.max_length)\n",
    "            do_sample = kwargs.get('do_sample', True)\n",
    "            output = self.generator(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample\n",
    "            )\n",
    "            return output[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def score(self, completions, **kwargs):\n",
    "        try:\n",
    "            scores = [self.log_likelihood(c, c) for c in completions]\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scoring: {e}\")\n",
    "            return [float(\"-inf\")] * len(completions)\n",
    "    \n",
    "    def log_likelihood(self, inputs, labels):\n",
    "        try:\n",
    "            inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            labels = tokenizer(labels, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            return -outputs.loss.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during log likelihood computation: {e}\")\n",
    "            return float(\"-inf\")\n",
    "\n",
    "# Configure DSPy Language Model\n",
    "lm = HuggingFaceLM(text_generator)\n",
    "\n",
    "# Optimized DSPy Configuration\n",
    "dspy.settings.configure(\n",
    "    lm=lm,\n",
    "    rm=colbertv2_wiki17_abstracts,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the Idea Summarization Signature\n",
    "class IdeaSummarizer(dspy.Signature):\n",
    "    \"\"\"Summarize ideas into concise, theme-aligned outputs.\"\"\"\n",
    "    idea = dspy.InputField(desc=\"A creative or philosophical idea to summarize.\")\n",
    "    themes = dspy.InputField(desc=\"Themes associated with this idea.\")\n",
    "    summary = dspy.OutputField(desc=\"A concise 1-5 word summary of the idea.\")\n",
    "\n",
    "# Define the Basic Question-Answering Signature\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "    question = dspy.InputField(desc=\"A question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"A concise answer between 1 and 5 words\")\n",
    "\n",
    "# Example pipeline for summarizing ideas\n",
    "example = devset[0]\n",
    "idea = example['Ideas']\n",
    "themes = \", \".join(filter(None, [example.get('Theme a'), example.get('Theme-b'), example.get('Theme-c')]))\n",
    "\n",
    "# Create prediction pipelines\n",
    "generate_summary = dspy.Predict(IdeaSummarizer)\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Generate summary and answers\n",
    "try:\n",
    "    result_summary = generate_summary(idea=idea, themes=themes)\n",
    "    print(\"Idea Summary:\")\n",
    "    print(f\"Idea: {idea}\")\n",
    "    print(f\"Themes: {themes}\")\n",
    "    print(f\"Summary: {result_summary.summary}\")\n",
    "    \n",
    "    result_answer = generate_answer(question=idea)\n",
    "    print(\"\\nQuestion Answer:\")\n",
    "    print(f\"Question: {idea}\")\n",
    "    print(f\"Answer: {result_answer.answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DSPy pipeline execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 119, Devset size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 98.19 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(huggingface_model)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create a text generation pipeline\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Improved Custom Language Model using DSPy LM\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHuggingFaceLM\u001b[39;00m(dspy\u001b[38;5;241m.\u001b[39mLM):\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1178\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:926\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    925\u001b[0m ):\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# If the model can generate, create a local generation config. This is done to avoid side-effects on the model\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# as we apply local tweaks to the generation config.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcan_generate():\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/mambaforge/envs/nunu24/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 3.43 GB, other allocations: 268.00 KB, max allowed: 3.40 GB). Tried to allocate 98.19 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "\n",
    "try:\n",
    "    # Load and preprocess the dataset\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')  # Drop unnecessary columns\n",
    "    data = data.dropna().reset_index(drop=True)  # Remove NaN values and reset index\n",
    "\n",
    "    # Convert to Hugging Face Dataset and split into train and test sets\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    trainset, devset = dataset['train'], dataset['test']\n",
    "\n",
    "    print(f\"Trainset size: {len(trainset)}, Devset size: {len(devset)}\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "\n",
    "# Load Hugging Face tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Improved Custom Language Model using DSPy LM\n",
    "class HuggingFaceLM(dspy.LM):\n",
    "    def __init__(self, generator, model=None, temperature=0.7, max_length=50):\n",
    "        model = model or generator.model\n",
    "        super().__init__(model=model)\n",
    "        self.generator = generator\n",
    "        self.temperature = temperature\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        if not prompt or not isinstance(prompt, str):\n",
    "            raise ValueError(\"Prompt must be a non-empty string.\")\n",
    "        return self.generate(prompt, **kwargs)\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        try:\n",
    "            temperature = kwargs.get('temperature', self.temperature)\n",
    "            max_length = kwargs.get('max_length', self.max_length)\n",
    "            do_sample = kwargs.get('do_sample', True)\n",
    "            output = self.generator(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample\n",
    "            )\n",
    "            return output[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def score(self, completions, **kwargs):\n",
    "        try:\n",
    "            scores = [self.log_likelihood(c, c) for c in completions]\n",
    "            return scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scoring: {e}\")\n",
    "            return [float(\"-inf\")] * len(completions)\n",
    "    \n",
    "    def log_likelihood(self, inputs, labels):\n",
    "        try:\n",
    "            inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            labels = tokenizer(labels, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            return -outputs.loss.item()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during log likelihood computation: {e}\")\n",
    "            return float(\"-inf\")\n",
    "\n",
    "# Configure DSPy Language Model\n",
    "lm = HuggingFaceLM(text_generator)\n",
    "\n",
    "# Optimized DSPy Configuration\n",
    "dspy.settings.configure(\n",
    "    lm=lm,\n",
    "    rm=colbertv2_wiki17_abstracts,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the Idea Summarization Signature\n",
    "class IdeaSummarizer(dspy.Signature):\n",
    "    \"\"\"Summarize ideas into concise, theme-aligned outputs.\"\"\"\n",
    "    idea = dspy.InputField(desc=\"A creative or philosophical idea to summarize.\")\n",
    "    themes = dspy.InputField(desc=\"Themes associated with this idea.\")\n",
    "    summary = dspy.OutputField(desc=\"A concise 1-5 word summary of the idea.\")\n",
    "\n",
    "# Define the Basic Question-Answering Signature\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "    question = dspy.InputField(desc=\"A question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"A concise answer between 1 and 5 words\")\n",
    "\n",
    "# Example pipeline for summarizing ideas\n",
    "example = devset[0]\n",
    "idea = example['Ideas']\n",
    "themes = \", \".join(filter(None, [example.get('Theme a'), example.get('Theme-b'), example.get('Theme-c')]))\n",
    "\n",
    "# Correct access to the 'question' field\n",
    "question = example['Ideas']  # Assuming the question is stored under the 'Ideas' field\n",
    "\n",
    "# Create prediction pipelines\n",
    "generate_summary = dspy.Predict(IdeaSummarizer)\n",
    "generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
    "\n",
    "# Generate summary and answers with chain of thought\n",
    "try:\n",
    "    result_summary = generate_summary(idea=idea, themes=themes)\n",
    "    print(\"Idea Summary:\")\n",
    "    print(f\"Idea: {idea}\")\n",
    "    print(f\"Themes: {themes}\")\n",
    "    print(f\"Summary: {result_summary.summary}\")\n",
    "    \n",
    "    # Correct access to the 'question' field\n",
    "    result_answer = generate_answer_with_chain_of_thought(question=example['Ideas'])  # Access 'Ideas' correctly\n",
    "    print(\"\\nQuestion Answer with Chain of Thought:\")\n",
    "    print(f\"Question: {example['Ideas']}\")\n",
    "    print(f\"Thought: {result_answer.rationale.split('.', 1)[1].strip()}\")\n",
    "    print(f\"Predicted Answer: {result_answer.answer}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DSPy pipeline execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">119</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m119\u001b[0m \u001b[1;36m30\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trainset Data <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'30) feminism - gitty up! Update 2-19-19 Feminism is important to me because I see half </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the population of the world suffering from a patriarchical prejudice that is stunting the growth of civilization. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">If that’s not enough to be a feminist, I don’t know what is. Prejudice is inherently dumb. Patriarchy is a biased </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">archaic system. Any ways I say gitty up because its little traveled territory for men to have a legit opinion on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feminist issues, but if you see something say something, right? Taking the concept of full expression as an example</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of maximized beauty, we see its extreme importance in all of our lives. Older but mildly edited writing \\uf0e0It’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">about comfort to express fully - inequality runs deep - share your beautiful life grrrl! Be all you and that’s all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you need to be satisfied… But for real tho sex positive feminism…. can’t say enough about it. You see in sexuality </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there’s a power of agenda (i.e. who sets the agenda is the powerful one). In that sense, feminist sexuality exists </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wherever women are controlling or have an equal role in the sexual agenda. I would also argue that feminist </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sexuality exists wherever women are satisfied with the sexual agenda or satisfied with their decision to engage in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a sexual act even if they are totally lacking any control of the agenda. Update 2-19-19 – Is there a backlash </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">against sex positivity in 2019? Could it be just a general distaste in men? Or could it be the influence of fascism</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the world? Are we subtly acting more fascist towards how we approach sex?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'118) BASIC like Blue and Red Try </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">overcome all the basic bitches that are so intertwined into the fabric of the US.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'89) DISCIPLINE -&gt; FEEL IT - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">That you should really feel the urges suppressed to really feel discipline to really discipline yourself. Does it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">also mean making yourself feel pain?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'77) Root out Negativity - Like Missy Elliot says “Let me search it, put my </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">thing down flip it and reverse it.” To root out negativity you gotta search for it in one way then put your thing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">down flip it and reverse it to really find that source of negativity and root it out…like really root it out.'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'81) X, Y Line Graph 4 All Things Like for example on the y line going vertical we got fun and on the x going we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">got productive. So like I’d probably rank higher in fun than productive whereas like book nerd Marcus is prolly a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit higher in Productivity. Hmmm there other ones too tho. Update 2-19-19 -&gt; All my art is this now. Its taken a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">big ol hold of my life. I love how it really creates a plan of emotion.'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span><span style=\"font-weight: bold\">]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trainset Data \u001b[1m{\u001b[0m\u001b[32m'Ideas'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'30\u001b[0m\u001b[32m)\u001b[0m\u001b[32m feminism - gitty up! Update 2-19-19 Feminism is important to me because I see half \u001b[0m\n",
       "\u001b[32mthe population of the world suffering from a patriarchical prejudice that is stunting the growth of civilization. \u001b[0m\n",
       "\u001b[32mIf that’s not enough to be a feminist, I don’t know what is. Prejudice is inherently dumb. Patriarchy is a biased \u001b[0m\n",
       "\u001b[32marchaic system. Any ways I say gitty up because its little traveled territory for men to have a legit opinion on \u001b[0m\n",
       "\u001b[32mfeminist issues, but if you see something say something, right? Taking the concept of full expression as an example\u001b[0m\n",
       "\u001b[32mof maximized beauty, we see its extreme importance in all of our lives. Older but mildly edited writing \\uf0e0It’s \u001b[0m\n",
       "\u001b[32mabout comfort to express fully - inequality runs deep - share your beautiful life grrrl! Be all you and that’s all \u001b[0m\n",
       "\u001b[32myou need to be satisfied… But for real tho sex positive feminism…. can’t say enough about it. You see in sexuality \u001b[0m\n",
       "\u001b[32mthere’s a power of agenda \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e. who sets the agenda is the powerful one\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In that sense, feminist sexuality exists \u001b[0m\n",
       "\u001b[32mwherever women are controlling or have an equal role in the sexual agenda. I would also argue that feminist \u001b[0m\n",
       "\u001b[32msexuality exists wherever women are satisfied with the sexual agenda or satisfied with their decision to engage in \u001b[0m\n",
       "\u001b[32ma sexual act even if they are totally lacking any control of the agenda. Update 2-19-19 – Is there a backlash \u001b[0m\n",
       "\u001b[32magainst sex positivity in 2019? Could it be just a general distaste in men? Or could it be the influence of fascism\u001b[0m\n",
       "\u001b[32min the world? Are we subtly acting more fascist towards how we approach sex?'\u001b[0m, \u001b[32m'118\u001b[0m\u001b[32m)\u001b[0m\u001b[32m BASIC like Blue and Red Try \u001b[0m\n",
       "\u001b[32movercome all the basic bitches that are so intertwined into the fabric of the US.'\u001b[0m, \u001b[32m'89\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DISCIPLINE -> FEEL IT - \u001b[0m\n",
       "\u001b[32mThat you should really feel the urges suppressed to really feel discipline to really discipline yourself. Does it \u001b[0m\n",
       "\u001b[32malso mean making yourself feel pain?'\u001b[0m, \u001b[32m'77\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Root out Negativity - Like Missy Elliot says “Let me search it, put my \u001b[0m\n",
       "\u001b[32mthing down flip it and reverse it.” To root out negativity you gotta search for it in one way then put your thing \u001b[0m\n",
       "\u001b[32mdown flip it and reverse it to really find that source of negativity and root it out…like really root it out.'\u001b[0m, \n",
       "\u001b[32m'81\u001b[0m\u001b[32m)\u001b[0m\u001b[32m X, Y Line Graph 4 All Things Like for example on the y line going vertical we got fun and on the x going we \u001b[0m\n",
       "\u001b[32mgot productive. So like I’d probably rank higher in fun than productive whereas like book nerd Marcus is prolly a \u001b[0m\n",
       "\u001b[32mbit higher in Productivity. Hmmm there other ones too tho. Update 2-19-19 -> All my art is this now. Its taken a \u001b[0m\n",
       "\u001b[32mbig ol hold of my life. I love how it really creates a plan of emotion.'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme a'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \n",
       "\u001b[32m'fun'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-b'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'rational'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-c'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'rational'\u001b[0m, \n",
       "\u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'inspiring'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Devset Data <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'19) take chances -&gt; shit where you eat - the babes we like are often those that hang with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our friends. The people we do business with are often are friends. Relats n biz get ugly often but sometimes you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can’t avoid shitting where you eat.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'59) Dbags fucking w/ you -&gt; They like puppies -&gt; You an old dog - The power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of pettiness aside (of that other set of 42 points), recognition of dbag tendencies should be the first bit of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wisdom used to help overcome that petty bullshit created by dbags. For real tho, dbags can get to you. One has to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be prepared for bullshit because it can be a bit tricky even in its most obviously recognizable forms. Why? Usually</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it’s created by those closest to us. I feel like something deeper can be said here, but maybe that’s for like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collaboration with some other person. '</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'144) creep nation - dude stop stalking me on creep book… isn’t it kinda </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">weird to think of your friends scrolling through you shit? Like what thoughts pop in their heads that make them </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">check your photos or job or likes?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'22) everyone has a void - i.e. - a hole - satisfaction is hard to come by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don’t feel bad that you feel bad'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'79) Vanity – Totally fucking you over? Does being a vain ass lil bitty always </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fuck you over? I’m not even sure? '</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span><span style=\"font-weight: bold\">]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Devset Data \u001b[1m{\u001b[0m\u001b[32m'Ideas'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'19\u001b[0m\u001b[32m)\u001b[0m\u001b[32m take chances -> shit where you eat - the babes we like are often those that hang with \u001b[0m\n",
       "\u001b[32mour friends. The people we do business with are often are friends. Relats n biz get ugly often but sometimes you \u001b[0m\n",
       "\u001b[32mcan’t avoid shitting where you eat.'\u001b[0m, \u001b[32m'59\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Dbags fucking w/ you -> They like puppies -> You an old dog - The power \u001b[0m\n",
       "\u001b[32mof pettiness aside \u001b[0m\u001b[32m(\u001b[0m\u001b[32mof that other set of 42 points\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, recognition of dbag tendencies should be the first bit of \u001b[0m\n",
       "\u001b[32mwisdom used to help overcome that petty bullshit created by dbags. For real tho, dbags can get to you. One has to \u001b[0m\n",
       "\u001b[32mbe prepared for bullshit because it can be a bit tricky even in its most obviously recognizable forms. Why? Usually\u001b[0m\n",
       "\u001b[32mit’s created by those closest to us. I feel like something deeper can be said here, but maybe that’s for like \u001b[0m\n",
       "\u001b[32mcollaboration with some other person. '\u001b[0m, \u001b[32m'144\u001b[0m\u001b[32m)\u001b[0m\u001b[32m creep nation - dude stop stalking me on creep book… isn’t it kinda \u001b[0m\n",
       "\u001b[32mweird to think of your friends scrolling through you shit? Like what thoughts pop in their heads that make them \u001b[0m\n",
       "\u001b[32mcheck your photos or job or likes?'\u001b[0m, \u001b[32m'22\u001b[0m\u001b[32m)\u001b[0m\u001b[32m everyone has a void - i.e. - a hole - satisfaction is hard to come by \u001b[0m\n",
       "\u001b[32mdon’t feel bad that you feel bad'\u001b[0m, \u001b[32m'79\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Vanity – Totally fucking you over? Does being a vain ass lil bitty always \u001b[0m\n",
       "\u001b[32mfuck you over? I’m not even sure? '\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme a'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-b'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'inspiring'\u001b[0m, \n",
       "\u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-c'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \n",
       "\u001b[32m'rational'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "dspy.settings.configure(lm=tokenizer, rm=colbertv2_wiki17_abstracts)  # Configure with tokenizer for simplicity\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    # Load CSV with proper column handling\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    \n",
    "    # Drop columns with unnamed or unnecessary data\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')\n",
    "    \n",
    "    # Drop rows with missing values and reset the index\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Convert the cleaned DataFrame to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)  # Split into train and validation sets\n",
    "\n",
    "    # Prepare the train and dev sets\n",
    "    trainset = dataset['train']\n",
    "    devset = dataset['test']\n",
    "\n",
    "    # Print the size and first few elements of each set\n",
    "    print(len(trainset), len(devset))\n",
    "    print(f\"Trainset Data {trainset[:5]}\")\n",
    "    print(f\"Devset Data {devset[:5]}\")\n",
    "\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2TokenizerFast' object has no attribute 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate Response\u001b[39;00m\n\u001b[1;32m     12\u001b[0m generate_answer \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(BasicQA)\n\u001b[0;32m---> 13\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredicted Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model from Hugging Face\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:154\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:171\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# If temperature is 0.0 but its n > 1, set temperature to 0.7.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m temperature \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m temperature\n\u001b[1;32m    172\u001b[0m num_generations \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_generations\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m temperature \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m num_generations \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2TokenizerFast' object has no attribute 'kwargs'"
     ]
    }
   ],
   "source": [
    "# Define BasicQA class\n",
    "class BasicQA(dspy.Signature):  # A. Signature\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Example Question with Answer\n",
    "example = devset[0]\n",
    "question = example['Ideas']\n",
    "\n",
    "# Generate Response\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "pred = generate_answer(question=question)\n",
    "print(f\"Question: {question}\\nPredicted Answer: {pred.answer}\")\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Tokenize the example\n",
    "inputs = tokenizer(question, return_tensors='pt')\n",
    "labels = tokenizer(pred.answer, return_tensors='pt')\n",
    "\n",
    "# Prepare dataset in Hugging Face format\n",
    "hf_dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "        'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "        'labels': [labels['input_ids'].squeeze().tolist()]\n",
    "    }),\n",
    "    'validation': Dataset.from_dict({\n",
    "        'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "        'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "        'labels': [labels['input_ids'].squeeze().tolist()]\n",
    "    })\n",
    "})\n",
    "\n",
    "# Print to verify\n",
    "print(hf_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
