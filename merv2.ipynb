{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using device: cpu\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using device: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cl2\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Debugging: Print Python and Library Versions\n",
    "import sys\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"DSPy Version:\", dspy.__version__)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    # Load CSV with extra diagnostics\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    print(\"\\nCSV Load Successful\")\n",
    "    print(\"CSV Columns:\", list(data.columns))\n",
    "    print(\"Total Rows:\", len(data))\n",
    "    print(\"\\nFirst few rows:\\n\", data.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CSV Loading Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Model and Tokenizer Loading with Extensive Debugging\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "try:\n",
    "    print(\"\\nLoading Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    print(\"Tokenizer Loaded Successfully\")\n",
    "\n",
    "    print(\"\\nLoading Model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model, \n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    ).to(device)\n",
    "    print(\"Model Loaded Successfully\")\n",
    "\n",
    "    print(\"\\nCreating Pipeline...\")\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if device == \"cuda\" else -1\n",
    "    )\n",
    "    print(\"Pipeline Created Successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model Loading Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Basic Pipeline Test\n",
    "try:\n",
    "    print(\"\\nTesting Text Generation...\")\n",
    "    test_prompt = \"Hello, how are you?\"\n",
    "    result = text_generator(test_prompt, max_length=50)\n",
    "    print(\"Generation Test Result:\")\n",
    "    print(result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Generation Test Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using device: cpu\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using device: cpu\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">CSV Data Columns:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 5'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "CSV Data Columns:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Ideas'\u001b[0m, \u001b[32m'Theme a'\u001b[0m, \u001b[32m'Theme-b'\u001b[0m, \u001b[32m'Theme-c'\u001b[0m, \u001b[32m'Unnamed: 4'\u001b[0m, \u001b[32m'Unnamed: 5'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">First few rows:\n",
       "                                                Ideas    Theme a    Theme-b  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span> Maximize the Beauty - fully channel the bea<span style=\"color: #808000; text-decoration-color: #808000\">...</span>        fun   rational   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span> Full Expression - it takes a lot of effort <span style=\"color: #808000; text-decoration-color: #808000\">...</span>  inspiring  intuitive   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span> - Neech real/dumb - Nietzsche had some grea<span style=\"color: #808000; text-decoration-color: #808000\">...</span>   negative      spicy   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span> - Camus - beautiful not pragmatic - I love <span style=\"color: #808000; text-decoration-color: #808000\">...</span>    opinion   rational   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">)</span> games – Be a good lil sociopath path and s<span style=\"color: #808000; text-decoration-color: #808000\">...</span>      rough      spicy   \n",
       "\n",
       "    Theme-c Unnamed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                                         Unnamed: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  positive   personal  theme ideas - rough, rational, intuitive, posi<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  positive   personal  top <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> idea themes- rational, positive, inspiri<span style=\"color: #808000; text-decoration-color: #808000\">...</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>     neech    opinion                                              phil   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>   neutral   opinion                                               phil   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>   emotion   rational                                          inspiring  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "First few rows:\n",
       "                                                Ideas    Theme a    Theme-b  \\\n",
       "\u001b[1;36m0\u001b[0m  \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m Maximize the Beauty - fully channel the bea\u001b[33m...\u001b[0m        fun   rational   \n",
       "\u001b[1;36m1\u001b[0m  \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m Full Expression - it takes a lot of effort \u001b[33m...\u001b[0m  inspiring  intuitive   \n",
       "\u001b[1;36m2\u001b[0m  \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m - Neech real/dumb - Nietzsche had some grea\u001b[33m...\u001b[0m   negative      spicy   \n",
       "\u001b[1;36m3\u001b[0m  \u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m - Camus - beautiful not pragmatic - I love \u001b[33m...\u001b[0m    opinion   rational   \n",
       "\u001b[1;36m4\u001b[0m  \u001b[1;36m16\u001b[0m\u001b[1m)\u001b[0m games – Be a good lil sociopath path and s\u001b[33m...\u001b[0m      rough      spicy   \n",
       "\n",
       "    Theme-c Unnamed: \u001b[1;36m4\u001b[0m                                         Unnamed: \u001b[1;36m5\u001b[0m  \n",
       "\u001b[1;36m0\u001b[0m  positive   personal  theme ideas - rough, rational, intuitive, posi\u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m  positive   personal  top \u001b[1;36m5\u001b[0m idea themes- rational, positive, inspiri\u001b[33m...\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m     neech    opinion                                              phil   \n",
       "\u001b[1;36m3\u001b[0m   neutral   opinion                                               phil   \n",
       "\u001b[1;36m4\u001b[0m   emotion   rational                                          inspiring  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DSPy Chatbot: Hello! I'm ready to help. Type <span style=\"color: #008000; text-decoration-color: #008000\">'exit'</span> to quit.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "DSPy Chatbot: Hello! I'm ready to help. Type \u001b[32m'exit'\u001b[0m to quit.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded dataset columns:\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 4'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Unnamed: 5'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loaded dataset columns:\n",
       "\u001b[1m[\u001b[0m\u001b[32m'Ideas'\u001b[0m, \u001b[32m'Theme a'\u001b[0m, \u001b[32m'Theme-b'\u001b[0m, \u001b[32m'Theme-c'\u001b[0m, \u001b[32m'Unnamed: 4'\u001b[0m, \u001b[32m'Unnamed: 5'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cl kinda works\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "import torch\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "\n",
    "    # Clean and prepare the data\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    print(\"CSV Data Columns:\", list(data.columns))\n",
    "    print(\"First few rows:\\n\", data.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load Hugging Face model and tokenizer\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model, \n",
    "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    # Create text generation pipeline with explicit device\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if device == \"cuda\" else -1  # use GPU if available\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Custom Language Model for DSPy\n",
    "class SimpleLLM(dspy.LM):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.generator(\n",
    "                prompt, \n",
    "                max_length=100, \n",
    "                num_return_sequences=1\n",
    "            )[0]['generated_text']\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"I'm having trouble generating a response.\"\n",
    "\n",
    "# Define a simple signature for the chatbot\n",
    "class ChatbotSignature(dspy.Signature):\n",
    "    \"\"\"Generate a helpful and concise response to a user's query.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    response = dspy.OutputField(desc=\"Helpful and relevant answer\")\n",
    "\n",
    "# Configure DSPy with the custom language model\n",
    "dspy.settings.configure(lm=SimpleLLM(text_generator))\n",
    "\n",
    "# Create a prediction module\n",
    "chatbot = dspy.Predict(ChatbotSignature)\n",
    "\n",
    "# Interactive chat loop\n",
    "def chat():\n",
    "    print(\"DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\")\n",
    "    print(\"Loaded dataset columns:\", list(data.columns))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            # Exit condition\n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"DSPy Chatbot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Enhance prompt with context from data if possible\n",
    "            # Assumes there's a column that might be relevant\n",
    "            context_column = data.columns[0] if len(data.columns) > 0 else None\n",
    "            \n",
    "            if context_column:\n",
    "                # Add some context from the dataset\n",
    "                enhanced_prompt = f\"Using information from the dataset, {user_input}\"\n",
    "            else:\n",
    "                enhanced_prompt = user_input\n",
    "            \n",
    "            # Generate response\n",
    "            response = chatbot(query=enhanced_prompt)\n",
    "            print(\"DSPy Chatbot:\", response.response)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {e}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\n",
      "Dataset columns: ['Ideas', 'Theme a', 'Theme-b', 'Theme-c', 'Unnamed: 4', 'Unnamed: 5']\n"
     ]
    }
   ],
   "source": [
    "# m2 claude optimized \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Optimized for M2 Mac\n",
    "huggingface_model = 'distilgpt2'  # Smaller, more lightweight model\n",
    "\n",
    "# Load the CSV\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "\n",
    "# Clean the data\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "# Load tokenizer and model with MPS (Metal Performance Shaders) support for M2\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    \n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Use MPS for M2 Mac\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Create text generation pipeline\n",
    "    text_generator = pipeline(\n",
    "        'text-generation', \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Model loading error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Custom Language Model for DSPy\n",
    "class SimpleLLM(dspy.LM):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "    \n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        try:\n",
    "            # Generate response\n",
    "            responses = self.generator(\n",
    "                prompt, \n",
    "                max_length=100, \n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            return responses[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"I'm having trouble generating a response.\"\n",
    "\n",
    "# Define chatbot signature\n",
    "class ChatbotSignature(dspy.Signature):\n",
    "    \"\"\"Generate a helpful and concise response to a user's query.\"\"\"\n",
    "    query = dspy.InputField()\n",
    "    response = dspy.OutputField(desc=\"Helpful and relevant answer\")\n",
    "\n",
    "# Configure DSPy\n",
    "dspy.settings.configure(lm=SimpleLLM(text_generator))\n",
    "\n",
    "# Create prediction module\n",
    "chatbot = dspy.Predict(ChatbotSignature)\n",
    "\n",
    "# Interactive chat loop\n",
    "def chat():\n",
    "    print(\"DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\")\n",
    "    print(\"Dataset columns:\", list(data.columns))\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \")\n",
    "            \n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"DSPy Chatbot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Add context from the dataset\n",
    "            context = data['Ideas'].sample(1).values[0]\n",
    "            enhanced_prompt = f\"Context from dataset: {context}\\n\\nUser query: {user_input}\"\n",
    "            \n",
    "       \n",
    "       \n",
    "            # Generate response\n",
    "            response = chatbot(query=enhanced_prompt)\n",
    "            print(\"DSPy Chatbot:\", response.response)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {e}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset size: 119, Devset size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during DSPy pipeline execution: 'HuggingFaceLM' object has no attribute 'cache'\n"
     ]
    }
   ],
   "source": [
    "# claude - 12-16-24\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "\n",
    "try:\n",
    "    # Load and preprocess the dataset\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')  # Drop unnecessary columns\n",
    "    data = data.dropna().reset_index(drop=True)  # Remove NaN values and reset index\n",
    "\n",
    "    # Convert to Hugging Face Dataset and split into train and test sets\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    trainset, devset = dataset['train'], dataset['test']\n",
    "\n",
    "    print(f\"Trainset size: {len(trainset)}, Devset size: {len(devset)}\")\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n",
    "\n",
    "# Load Hugging Face tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Create a text generation pipeline\n",
    "text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Custom Language Model using DSPy LM\n",
    "class HuggingFaceLM(dspy.LM):\n",
    "    def __init__(self, generator, **kwargs):\n",
    "        self.generator = generator\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def generate(self, prompt, **kwargs):\n",
    "        \"\"\"Generate output text given a prompt.\"\"\"\n",
    "        try:\n",
    "            # Merge any additional kwargs with defaults\n",
    "            final_kwargs = {**self.kwargs, **kwargs}\n",
    "            output = self.generator(prompt, max_length=50, **final_kwargs)\n",
    "            return output[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# Configure DSPy settings\n",
    "dspy.settings.configure(\n",
    "    lm=HuggingFaceLM(text_generator, temperature=0.7),  # Custom Hugging Face LM\n",
    "    rm=colbertv2_wiki17_abstracts                        # Retrieval model\n",
    ")\n",
    "\n",
    "# Define the Basic Question-Answering Signature\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factual answers.\"\"\"\n",
    "    question = dspy.InputField(desc=\"A question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"A concise answer between 1 and 5 words\")\n",
    "\n",
    "# Example pipeline to generate predictions\n",
    "example = devset[0]\n",
    "question = example['Ideas']\n",
    "\n",
    "# Define DSPy Prediction Pipeline\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "# Execute pipeline with an example question\n",
    "try:\n",
    "    result = generate_answer(question=question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {result.answer}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during DSPy pipeline execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LMWithKwargs' object has no attribute 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Generate Response with tokenized question\u001b[39;00m\n\u001b[1;32m     33\u001b[0m generate_answer \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(BasicQA)\n\u001b[0;32m---> 34\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredicted Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Tokenize the predicted answer (if it's a string)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:154\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:171\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# If temperature is 0.0 but its n > 1, set temperature to 0.7.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m temperature \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m temperature\n\u001b[1;32m    172\u001b[0m num_generations \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_generations\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m temperature \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m num_generations \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LMWithKwargs' object has no attribute 'kwargs'"
     ]
    }
   ],
   "source": [
    "# g2 \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Example Question with Answer\n",
    "example = devset[0]\n",
    "question = example['Ideas']\n",
    "\n",
    "# Create the kwargs wrapper (WITHOUT temperature)\n",
    "class LMWithKwargs:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.model(*args) # call with just *args\n",
    "\n",
    "# Load the actual language model and wrap it\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "lm_with_kwargs = LMWithKwargs(model)\n",
    "\n",
    "# Now configure dspy with the wrapper\n",
    "dspy.settings.configure(lm=lm_with_kwargs, rm=colbertv2_wiki17_abstracts)\n",
    "\n",
    "# Load tokenizer from Hugging Face (needed for tokenization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "\n",
    "# Tokenize the question\n",
    "question_ids = tokenizer(question, return_tensors='pt').input_ids\n",
    "question_text = tokenizer.decode(question_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "# Generate Response with tokenized question\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "pred = generate_answer(question=question_text)\n",
    "print(f\"Question: {question}\\nPredicted Answer: {pred.answer}\")\n",
    "\n",
    "# Tokenize the predicted answer (if it's a string)\n",
    "if isinstance(pred.answer, str): #check if its a string\n",
    "    labels = tokenizer(pred.answer, return_tensors='pt').input_ids\n",
    "elif isinstance(pred.answer, list): #check if its a list\n",
    "    labels = tokenizer(pred.answer[0], return_tensors='pt').input_ids\n",
    "else:\n",
    "    labels = None\n",
    "    print(\"pred.answer is not a string or list\")\n",
    "# Tokenize the example\n",
    "inputs = tokenizer(question, return_tensors='pt')\n",
    "\n",
    "# Prepare dataset in Hugging Face format (only if labels is not None)\n",
    "if labels is not None:\n",
    "    hf_dataset = DatasetDict({\n",
    "        'train': Dataset.from_dict({\n",
    "            'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "            'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "            'labels': [labels.squeeze().tolist()]\n",
    "        }),\n",
    "        'validation': Dataset.from_dict({\n",
    "            'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "            'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "            'labels': [labels.squeeze().tolist()]\n",
    "        })\n",
    "    })\n",
    "\n",
    "    # Print to verify\n",
    "    print(hf_dataset)\n",
    "else:\n",
    "    print(\"Skipping hf_dataset creation due to invalid labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "### Generate Response with Chain of Thought ###\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "### Generate Response with Chain of Thought ###\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m### Generate Response with Chain of Thought ###\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m generate_answer_with_chain_of_thought \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mChainOfThought(BasicQA)\n\u001b[0;32m----> 6\u001b[0m pred \u001b[38;5;241m=\u001b[39m generate_answer_with_chain_of_thought(question\u001b[38;5;241m=\u001b[39m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;241m.\u001b[39mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThought: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mrationale\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredicted Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'question'"
     ]
    }
   ],
   "source": [
    "# chain gem\n",
    "\n",
    "# 3. Chatbot with Chain of Thought\n",
    "print(\"\\n### Generate Response with Chain of Thought ###\\n\")\n",
    "generate_answer_with_chain_of_thought = dspy.ChainOfThought(BasicQA)\n",
    "pred = generate_answer_with_chain_of_thought(question=example.question)\n",
    "print(f\"Question: {example.question}\\nThought: {pred.rationale.split('.', 1)[1].strip()}\\nPredicted Answer: {pred.answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">119</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m119\u001b[0m \u001b[1;36m30\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Trainset Data <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'30) feminism - gitty up! Update 2-19-19 Feminism is important to me because I see half </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the population of the world suffering from a patriarchical prejudice that is stunting the growth of civilization. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">If that’s not enough to be a feminist, I don’t know what is. Prejudice is inherently dumb. Patriarchy is a biased </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">archaic system. Any ways I say gitty up because its little traveled territory for men to have a legit opinion on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">feminist issues, but if you see something say something, right? Taking the concept of full expression as an example</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of maximized beauty, we see its extreme importance in all of our lives. Older but mildly edited writing \\uf0e0It’s </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">about comfort to express fully - inequality runs deep - share your beautiful life grrrl! Be all you and that’s all </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you need to be satisfied… But for real tho sex positive feminism…. can’t say enough about it. You see in sexuality </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there’s a power of agenda (i.e. who sets the agenda is the powerful one). In that sense, feminist sexuality exists </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wherever women are controlling or have an equal role in the sexual agenda. I would also argue that feminist </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sexuality exists wherever women are satisfied with the sexual agenda or satisfied with their decision to engage in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a sexual act even if they are totally lacking any control of the agenda. Update 2-19-19 – Is there a backlash </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">against sex positivity in 2019? Could it be just a general distaste in men? Or could it be the influence of fascism</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">in the world? Are we subtly acting more fascist towards how we approach sex?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'118) BASIC like Blue and Red Try </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">overcome all the basic bitches that are so intertwined into the fabric of the US.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'89) DISCIPLINE -&gt; FEEL IT - </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">That you should really feel the urges suppressed to really feel discipline to really discipline yourself. Does it </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">also mean making yourself feel pain?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'77) Root out Negativity - Like Missy Elliot says “Let me search it, put my </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">thing down flip it and reverse it.” To root out negativity you gotta search for it in one way then put your thing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">down flip it and reverse it to really find that source of negativity and root it out…like really root it out.'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'81) X, Y Line Graph 4 All Things Like for example on the y line going vertical we got fun and on the x going we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">got productive. So like I’d probably rank higher in fun than productive whereas like book nerd Marcus is prolly a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">bit higher in Productivity. Hmmm there other ones too tho. Update 2-19-19 -&gt; All my art is this now. Its taken a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">big ol hold of my life. I love how it really creates a plan of emotion.'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span><span style=\"font-weight: bold\">]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Trainset Data \u001b[1m{\u001b[0m\u001b[32m'Ideas'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'30\u001b[0m\u001b[32m)\u001b[0m\u001b[32m feminism - gitty up! Update 2-19-19 Feminism is important to me because I see half \u001b[0m\n",
       "\u001b[32mthe population of the world suffering from a patriarchical prejudice that is stunting the growth of civilization. \u001b[0m\n",
       "\u001b[32mIf that’s not enough to be a feminist, I don’t know what is. Prejudice is inherently dumb. Patriarchy is a biased \u001b[0m\n",
       "\u001b[32marchaic system. Any ways I say gitty up because its little traveled territory for men to have a legit opinion on \u001b[0m\n",
       "\u001b[32mfeminist issues, but if you see something say something, right? Taking the concept of full expression as an example\u001b[0m\n",
       "\u001b[32mof maximized beauty, we see its extreme importance in all of our lives. Older but mildly edited writing \\uf0e0It’s \u001b[0m\n",
       "\u001b[32mabout comfort to express fully - inequality runs deep - share your beautiful life grrrl! Be all you and that’s all \u001b[0m\n",
       "\u001b[32myou need to be satisfied… But for real tho sex positive feminism…. can’t say enough about it. You see in sexuality \u001b[0m\n",
       "\u001b[32mthere’s a power of agenda \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e. who sets the agenda is the powerful one\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In that sense, feminist sexuality exists \u001b[0m\n",
       "\u001b[32mwherever women are controlling or have an equal role in the sexual agenda. I would also argue that feminist \u001b[0m\n",
       "\u001b[32msexuality exists wherever women are satisfied with the sexual agenda or satisfied with their decision to engage in \u001b[0m\n",
       "\u001b[32ma sexual act even if they are totally lacking any control of the agenda. Update 2-19-19 – Is there a backlash \u001b[0m\n",
       "\u001b[32magainst sex positivity in 2019? Could it be just a general distaste in men? Or could it be the influence of fascism\u001b[0m\n",
       "\u001b[32min the world? Are we subtly acting more fascist towards how we approach sex?'\u001b[0m, \u001b[32m'118\u001b[0m\u001b[32m)\u001b[0m\u001b[32m BASIC like Blue and Red Try \u001b[0m\n",
       "\u001b[32movercome all the basic bitches that are so intertwined into the fabric of the US.'\u001b[0m, \u001b[32m'89\u001b[0m\u001b[32m)\u001b[0m\u001b[32m DISCIPLINE -> FEEL IT - \u001b[0m\n",
       "\u001b[32mThat you should really feel the urges suppressed to really feel discipline to really discipline yourself. Does it \u001b[0m\n",
       "\u001b[32malso mean making yourself feel pain?'\u001b[0m, \u001b[32m'77\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Root out Negativity - Like Missy Elliot says “Let me search it, put my \u001b[0m\n",
       "\u001b[32mthing down flip it and reverse it.” To root out negativity you gotta search for it in one way then put your thing \u001b[0m\n",
       "\u001b[32mdown flip it and reverse it to really find that source of negativity and root it out…like really root it out.'\u001b[0m, \n",
       "\u001b[32m'81\u001b[0m\u001b[32m)\u001b[0m\u001b[32m X, Y Line Graph 4 All Things Like for example on the y line going vertical we got fun and on the x going we \u001b[0m\n",
       "\u001b[32mgot productive. So like I’d probably rank higher in fun than productive whereas like book nerd Marcus is prolly a \u001b[0m\n",
       "\u001b[32mbit higher in Productivity. Hmmm there other ones too tho. Update 2-19-19 -> All my art is this now. Its taken a \u001b[0m\n",
       "\u001b[32mbig ol hold of my life. I love how it really creates a plan of emotion.'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme a'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \n",
       "\u001b[32m'fun'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-b'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'rational'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-c'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'rational'\u001b[0m, \n",
       "\u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'inspiring'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Devset Data <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'Ideas'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'19) take chances -&gt; shit where you eat - the babes we like are often those that hang with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">our friends. The people we do business with are often are friends. Relats n biz get ugly often but sometimes you </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can’t avoid shitting where you eat.'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'59) Dbags fucking w/ you -&gt; They like puppies -&gt; You an old dog - The power </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of pettiness aside (of that other set of 42 points), recognition of dbag tendencies should be the first bit of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">wisdom used to help overcome that petty bullshit created by dbags. For real tho, dbags can get to you. One has to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">be prepared for bullshit because it can be a bit tricky even in its most obviously recognizable forms. Why? Usually</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">it’s created by those closest to us. I feel like something deeper can be said here, but maybe that’s for like </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">collaboration with some other person. '</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'144) creep nation - dude stop stalking me on creep book… isn’t it kinda </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">weird to think of your friends scrolling through you shit? Like what thoughts pop in their heads that make them </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">check your photos or job or likes?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'22) everyone has a void - i.e. - a hole - satisfaction is hard to come by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">don’t feel bad that you feel bad'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'79) Vanity – Totally fucking you over? Does being a vain ass lil bitty always </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fuck you over? I’m not even sure? '</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme a'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'fun'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-b'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'inspiring'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Theme-c'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'rational'</span><span style=\"font-weight: bold\">]}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Devset Data \u001b[1m{\u001b[0m\u001b[32m'Ideas'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'19\u001b[0m\u001b[32m)\u001b[0m\u001b[32m take chances -> shit where you eat - the babes we like are often those that hang with \u001b[0m\n",
       "\u001b[32mour friends. The people we do business with are often are friends. Relats n biz get ugly often but sometimes you \u001b[0m\n",
       "\u001b[32mcan’t avoid shitting where you eat.'\u001b[0m, \u001b[32m'59\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Dbags fucking w/ you -> They like puppies -> You an old dog - The power \u001b[0m\n",
       "\u001b[32mof pettiness aside \u001b[0m\u001b[32m(\u001b[0m\u001b[32mof that other set of 42 points\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, recognition of dbag tendencies should be the first bit of \u001b[0m\n",
       "\u001b[32mwisdom used to help overcome that petty bullshit created by dbags. For real tho, dbags can get to you. One has to \u001b[0m\n",
       "\u001b[32mbe prepared for bullshit because it can be a bit tricky even in its most obviously recognizable forms. Why? Usually\u001b[0m\n",
       "\u001b[32mit’s created by those closest to us. I feel like something deeper can be said here, but maybe that’s for like \u001b[0m\n",
       "\u001b[32mcollaboration with some other person. '\u001b[0m, \u001b[32m'144\u001b[0m\u001b[32m)\u001b[0m\u001b[32m creep nation - dude stop stalking me on creep book… isn’t it kinda \u001b[0m\n",
       "\u001b[32mweird to think of your friends scrolling through you shit? Like what thoughts pop in their heads that make them \u001b[0m\n",
       "\u001b[32mcheck your photos or job or likes?'\u001b[0m, \u001b[32m'22\u001b[0m\u001b[32m)\u001b[0m\u001b[32m everyone has a void - i.e. - a hole - satisfaction is hard to come by \u001b[0m\n",
       "\u001b[32mdon’t feel bad that you feel bad'\u001b[0m, \u001b[32m'79\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Vanity – Totally fucking you over? Does being a vain ass lil bitty always \u001b[0m\n",
       "\u001b[32mfuck you over? I’m not even sure? '\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme a'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m, \u001b[32m'fun'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-b'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'inspiring'\u001b[0m, \n",
       "\u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m, \u001b[32m'inspiring'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'Theme-c'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \u001b[32m'rational'\u001b[0m, \n",
       "\u001b[32m'rational'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import dspy\n",
    "\n",
    "# Configuration & Data Loading\n",
    "huggingface_model = 'facebook/opt-350m'\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "dspy.settings.configure(lm=tokenizer, rm=colbertv2_wiki17_abstracts)  # Configure with tokenizer for simplicity\n",
    "\n",
    "# Load and clean your CSV dataset\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "try:\n",
    "    # Load CSV with proper column handling\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    \n",
    "    # Drop columns with unnamed or unnecessary data\n",
    "    data = data.drop(columns=['Unnamed: 4', 'Unnamed: 5'], errors='ignore')\n",
    "    \n",
    "    # Drop rows with missing values and reset the index\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Convert the cleaned DataFrame to a Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    dataset = dataset.train_test_split(test_size=0.2)  # Split into train and validation sets\n",
    "\n",
    "    # Prepare the train and dev sets\n",
    "    trainset = dataset['train']\n",
    "    devset = dataset['test']\n",
    "\n",
    "    # Print the size and first few elements of each set\n",
    "    print(len(trainset), len(devset))\n",
    "    print(f\"Trainset Data {trainset[:5]}\")\n",
    "    print(f\"Devset Data {devset[:5]}\")\n",
    "\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2TokenizerFast' object has no attribute 'kwargs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate Response\u001b[39;00m\n\u001b[1;32m     12\u001b[0m generate_answer \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPredict(BasicQA)\n\u001b[0;32m---> 13\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPredicted Answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Load tokenizer and model from Hugging Face\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:154\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:171\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# If temperature is 0.0 but its n > 1, set temperature to 0.7.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m temperature \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m temperature\n\u001b[1;32m    172\u001b[0m num_generations \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_generations\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m temperature \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.15\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m num_generations \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2TokenizerFast' object has no attribute 'kwargs'"
     ]
    }
   ],
   "source": [
    "# Define BasicQA class\n",
    "class BasicQA(dspy.Signature):  # A. Signature\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
    "\n",
    "# Example Question with Answer\n",
    "example = devset[0]\n",
    "question = example['Ideas']\n",
    "\n",
    "# Generate Response\n",
    "generate_answer = dspy.Predict(BasicQA)\n",
    "pred = generate_answer(question=question)\n",
    "print(f\"Question: {question}\\nPredicted Answer: {pred.answer}\")\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(huggingface_model)\n",
    "\n",
    "# Tokenize the example\n",
    "inputs = tokenizer(question, return_tensors='pt')\n",
    "labels = tokenizer(pred.answer, return_tensors='pt')\n",
    "\n",
    "# Prepare dataset in Hugging Face format\n",
    "hf_dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "        'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "        'labels': [labels['input_ids'].squeeze().tolist()]\n",
    "    }),\n",
    "    'validation': Dataset.from_dict({\n",
    "        'input_ids': [inputs['input_ids'].squeeze().tolist()],\n",
    "        'attention_mask': [inputs['attention_mask'].squeeze().tolist()],\n",
    "        'labels': [labels['input_ids'].squeeze().tolist()]\n",
    "    })\n",
    "})\n",
    "\n",
    "# Print to verify\n",
    "print(hf_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
