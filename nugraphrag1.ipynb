{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Knowledge graph created and saved.\n",
      "Query: How can mindfulness improve well-being and inner beauty?\n",
      "Relevant Knowledge:\n",
      " - inner_beauty\n",
      " - well_being\n",
      " - mindfulness\n",
      " - inner_beauty promotes mindfulness\n",
      " - inner_beauty enhances well_being\n",
      " - well_being improves mindfulness\n",
      " - well_being enhances inner_beauty\n",
      " - mindfulness promotes inner_beauty\n",
      " - mindfulness improves well_being\n",
      "- mindfulness promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_beauty\n",
      "- mindfulness promotes inner_beauty promotes inner_\n"
     ]
    }
   ],
   "source": [
    "# nugraphrag1\n",
    "import torch\n",
    "import networkx as nx\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize GPT-2 Model\n",
    "def initialize_model():\n",
    "    \"\"\"Initialize the tokenizer and GPT-2 model.\"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")  # Set padding_side to 'left'\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\").half()  # Load GPT-2 in half-precision\n",
    "    model.to(device)  # Make sure it's on the correct device\n",
    "\n",
    "    # Set pad_token (GPT-2 doesn't have one by default, so we use eos_token as pad_token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Step 1: Create a Knowledge Graph (Example with NetworkX)\n",
    "def create_knowledge_graph():\n",
    "    \"\"\"Create a sample knowledge graph.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Example nodes (entities) and edges (relationships)\n",
    "    G.add_node(\"inner_beauty\")\n",
    "    G.add_node(\"mindfulness\")\n",
    "    G.add_node(\"well_being\")\n",
    "    \n",
    "    G.add_edge(\"inner_beauty\", \"mindfulness\", relationship=\"promotes\")\n",
    "    G.add_edge(\"mindfulness\", \"well_being\", relationship=\"improves\")\n",
    "    G.add_edge(\"inner_beauty\", \"well_being\", relationship=\"enhances\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Step 2: Retrieve Knowledge from Graph\n",
    "def retrieve_from_graph(graph, query):\n",
    "    \"\"\"Retrieve relevant information from the knowledge graph based on the input query.\"\"\"\n",
    "    relevant_nodes = []\n",
    "    relevant_edges = []\n",
    "    \n",
    "    # Simple logic to find relevant nodes (could be improved with NLP techniques)\n",
    "    if \"beauty\" in query:\n",
    "        relevant_nodes.append(\"inner_beauty\")\n",
    "        relevant_nodes.append(\"well_being\")\n",
    "    if \"mindfulness\" in query:\n",
    "        relevant_nodes.append(\"mindfulness\")\n",
    "    \n",
    "    # Retrieve edges based on relevant nodes\n",
    "    for node in relevant_nodes:\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        for neighbor in neighbors:\n",
    "            edge = graph.get_edge_data(node, neighbor)\n",
    "            relevant_edges.append((node, neighbor, edge))\n",
    "    \n",
    "    return relevant_nodes, relevant_edges\n",
    "\n",
    "# Step 3: Use Graph Knowledge to Generate Context-Aware Responses with GPT-2\n",
    "def generate_with_graph_knowledge(query, model, tokenizer, graph):\n",
    "    \"\"\"Generate a response based on a query and relevant knowledge from the graph.\"\"\"\n",
    "    # Retrieve relevant nodes and edges\n",
    "    relevant_nodes, relevant_edges = retrieve_from_graph(graph, query)\n",
    "    \n",
    "    # Format the retrieved knowledge into a context string\n",
    "    context = f\"Query: {query}\\n\"\n",
    "    context += f\"Relevant Knowledge:\\n\"\n",
    "    \n",
    "    for node in relevant_nodes:\n",
    "        context += f\" - {node}\\n\"\n",
    "    \n",
    "    for edge in relevant_edges:\n",
    "        context += f\" - {edge[0]} {edge[2]['relationship']} {edge[1]}\\n\"\n",
    "    \n",
    "    # Use GPT-2 to generate a response with the enriched context\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=200, \n",
    "        num_return_sequences=1, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Save and Load Knowledge Graph\n",
    "def save_graph(graph, filepath=\"knowledge_graph.pkl\"):\n",
    "    \"\"\"Save the knowledge graph to a file.\"\"\"\n",
    "    import pickle\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "def load_graph(filepath=\"knowledge_graph.pkl\"):\n",
    "    \"\"\"Load the knowledge graph from a file.\"\"\"\n",
    "    import pickle\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Main Processing Function\n",
    "def main():\n",
    "    \"\"\"Main processing loop.\"\"\"\n",
    "    # Filepath to save/load the graph\n",
    "    save_path = \"knowledge_graph.pkl\"\n",
    "    \n",
    "    # Load or create the knowledge graph\n",
    "    try:\n",
    "        graph = load_graph(save_path)\n",
    "        print(\"Knowledge graph loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        graph = create_knowledge_graph()\n",
    "        save_graph(graph, save_path)\n",
    "        print(\"Knowledge graph created and saved.\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    tokenizer, model = initialize_model()\n",
    "\n",
    "    # Example query\n",
    "    query = \"How can mindfulness improve well-being and inner beauty?\"\n",
    "\n",
    "    # Generate a response using the graph knowledge\n",
    "    response = generate_with_graph_knowledge(query, model, tokenizer, graph)\n",
    "\n",
    "    # Return the generated response\n",
    "    return response\n",
    "\n",
    "# Example Execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        output = main()  # Process the query and generate a response\n",
    "        print(output)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
