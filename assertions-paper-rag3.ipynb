{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Query: As you can see, the following keywords are added t\n",
      "Generated Query: This is usually done through keyword search, but a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      "\n",
      "\n",
      "Suggestions for Improvement:\n",
      "No results found, so suggestions cannot be provided.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    Custom Language Model wrapper for Hugging Face transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilgpt2'):  # Use a lightweight model for efficiency\n",
    "        self.generator = pipeline('text-generation', \n",
    "                                  model=model_name, \n",
    "                                  device=0,  # Use GPU if available\n",
    "                                  truncation=True)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        response = self.generator(\n",
    "            prompt, \n",
    "            max_length=len(prompt) + 250,   # Shorter max length\n",
    "            num_return_sequences=1,\n",
    "            **kwargs\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Clean up the generated text\n",
    "        response = response.replace(prompt, '').strip()\n",
    "        return response\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        # Load CSV data with more robust parsing\n",
    "        self.df = pd.read_csv(csv_path, \n",
    "                               quotechar='\"', \n",
    "                               escapechar='\\\\', \n",
    "                               skipinitialspace=True,\n",
    "                               on_bad_lines='skip')\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        # Verify the column exists\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "        # Initialize the language model\n",
    "        self.language_model = HuggingFaceLanguageModel(model_name=model_name)\n",
    "\n",
    "    def generate_query(self, context: list, question: str):\n",
    "        \"\"\"\n",
    "        Generate a query using context and a question.\n",
    "        \"\"\"\n",
    "        # Specific prompt to generate a focused query\n",
    "        context_str = str(context) if context else \"No previous context\"\n",
    "        prompt = (\n",
    "            f\"Context: {context_str}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Generate a very short, specific keyword or phrase to search for relevant information. \"\n",
    "            \"Focus on the key concept directly related to the question.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def safe_contains(self, text, query):\n",
    "        \"\"\"\n",
    "        Safely check if query is in text, using simple string matching\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        \n",
    "        # Use simple case-insensitive substring search\n",
    "        return query.lower() in text.lower()\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieve data from the CSV based on the generated query.\n",
    "        \"\"\"\n",
    "        # Use a custom search function to avoid regex parsing issues\n",
    "        results = self.df[\n",
    "            self.df[self.column_to_search].apply(\n",
    "                lambda x: self.safe_contains(str(x), query)\n",
    "            )\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def generate_answer(self, context: pd.DataFrame, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer based on retrieved context.\n",
    "        \"\"\"\n",
    "        # Truncate context if it's too large\n",
    "        context_text = context.head(5).to_string(index=False)\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Context: {context_text}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Based on the given context, provide a clear and concise answer. \"\n",
    "            \"Focus on explaining the key concept directly related to the question.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        \"\"\"\n",
    "        Multi-hop process for query generation, retrieval, and summarization.\n",
    "        \"\"\"\n",
    "        context, queries = [], [question]\n",
    "\n",
    "        for hop in range(2):\n",
    "            # Generate a query based on existing context\n",
    "            query = self.generate_query(context=context, question=question)\n",
    "            \n",
    "            # Ensure query is not empty or too similar\n",
    "            query = query[:50]  # Truncate to reasonable length\n",
    "            if query.lower() in [q.lower() for q in queries]:\n",
    "                query = question.split()[:2]  # Fallback to first two words\n",
    "            \n",
    "            print(f\"Generated Query: {query}\")\n",
    "            \n",
    "            # Retrieve results and add to context\n",
    "            hop_results = self.retrieve(query)\n",
    "            if not hop_results.empty:\n",
    "                context.append(hop_results)\n",
    "            queries.append(query)\n",
    "\n",
    "        # Generate final answer from collected context\n",
    "        context_df = pd.concat(context).drop_duplicates() if context else self.df\n",
    "        return self.generate_answer(context=context_df, question=question)\n",
    "\n",
    "    def suggest_improvements(self, query: str):\n",
    "        \"\"\"\n",
    "        Suggest improvements to refine the search query.\n",
    "        \"\"\"\n",
    "        # Perform a search and collect context\n",
    "        context = self.retrieve(query)\n",
    "\n",
    "        if context.empty:\n",
    "            return {\"query\": query, \"suggestions\": \"No results found, so suggestions cannot be provided.\"}\n",
    "\n",
    "        # Generate improvement suggestions\n",
    "        prompt = (\n",
    "            f\"Context: {context.to_string(index=False)}\\n\"\n",
    "            f\"Past Query: {query}\\n\"\n",
    "            \"Instruction: Suggest ways to improve or refine the search query for better results.\"\n",
    "        )\n",
    "        suggestions = self.language_model(prompt)\n",
    "        return {\"query\": query, \"suggestions\": suggestions}\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "csv_path = './151_ideas_updated.csv'\n",
    "column_to_search = 'Ideas'  # Adjust to the appropriate column\n",
    "processor = DataProcessor(csv_path=csv_path, column_to_search=column_to_search)\n",
    "\n",
    "# Multi-hop query generation and answering\n",
    "question = \"what is it to Maximize the Beauty?\"\n",
    "answer = processor.forward(question)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(answer)\n",
    "\n",
    "# Suggestions for query improvement\n",
    "improvement_suggestions = processor.suggest_improvements(question)\n",
    "print(\"\\nSuggestions for Improvement:\")\n",
    "print(improvement_suggestions[\"suggestions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 110, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./151_ideas_updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    116\u001b[0m column_to_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdeas\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust to the appropriate column\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDataProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_to_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_to_search\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Multi-hop query generation and answering\u001b[39;00m\n\u001b[1;32m    120\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximize the Beauty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mDataProcessor.__init__\u001b[0;34m(self, csv_path, column_to_search, model_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path: \u001b[38;5;28mstr\u001b[39m, column_to_search: \u001b[38;5;28mstr\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Load CSV data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_to_search \u001b[38;5;241m=\u001b[39m column_to_search\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Verify the column exists\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 110, saw 9\n"
     ]
    }
   ],
   "source": [
    "# from docs rag 2\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    Custom Language Model wrapper for Hugging Face transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilgpt2'):  # Use a lightweight model for efficiency\n",
    "        self.generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        response = self.generator(prompt, max_length=150, **kwargs)[0]['generated_text']\n",
    "        return response\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        # Load CSV data\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        # Verify the column exists\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "        # Initialize the language model\n",
    "        self.language_model = HuggingFaceLanguageModel(model_name=model_name)\n",
    "\n",
    "    def generate_query(self, context: list, question: str):\n",
    "        \"\"\"\n",
    "        Generate a query using context and a question.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Context: {context}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Generate a distinct and concise query to retrieve relevant data.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieve data from the CSV based on the generated query.\n",
    "        \"\"\"\n",
    "        results = self.df[self.df[self.column_to_search].str.contains(query, case=False, na=False)]\n",
    "        return results\n",
    "\n",
    "    def generate_answer(self, context: pd.DataFrame, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer based on retrieved context.\n",
    "        \"\"\"\n",
    "        context_text = context.to_string(index=False)\n",
    "        prompt = (\n",
    "            f\"Context: {context_text}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Provide a detailed summary of the retrieved data.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        \"\"\"\n",
    "        Multi-hop process for query generation, retrieval, and summarization.\n",
    "        \"\"\"\n",
    "        context, queries = [], [question]\n",
    "\n",
    "        for hop in range(2):\n",
    "            query = self.generate_query(context=context, question=question)\n",
    "            \n",
    "            # Assertions and suggestions\n",
    "            if len(query) >= 100:\n",
    "                print(\"FAIL! ✗ Query too long. Regenerating with updated prompt...\")\n",
    "                query = self.generate_query(context=context, question=f\"{question} (Keep it concise)\")\n",
    "\n",
    "            if query in queries:\n",
    "                print(f\"FAIL! ✗ Query not distinct from previous attempts. Regenerating...\")\n",
    "                query = self.generate_query(\n",
    "                    context=context,\n",
    "                    question=f\"{question} (Avoid similarity with previous attempts: {queries})\"\n",
    "                )\n",
    "\n",
    "            print(f\"Generated Query: {query}\")\n",
    "            context.append(self.retrieve(query))\n",
    "            queries.append(query)\n",
    "\n",
    "        # Generate final answer from collected context\n",
    "        context_df = pd.concat(context).drop_duplicates()\n",
    "        return self.generate_answer(context=context_df, question=question)\n",
    "\n",
    "    def suggest_improvements(self, query: str):\n",
    "        \"\"\"\n",
    "        Suggest improvements to refine the search query.\n",
    "        \"\"\"\n",
    "        # Perform a search and collect context\n",
    "        context = self.retrieve(query)\n",
    "\n",
    "        if context.empty:\n",
    "            return {\"query\": query, \"suggestions\": \"No results found, so suggestions cannot be provided.\"}\n",
    "\n",
    "        # Generate improvement suggestions\n",
    "        prompt = (\n",
    "            f\"Context: {context.to_string(index=False)}\\n\"\n",
    "            f\"Past Query: {query}\\n\"\n",
    "            \"Instruction: Suggest ways to improve or refine the search query for better results.\"\n",
    "        )\n",
    "        suggestions = self.language_model(prompt)\n",
    "        return {\"query\": query, \"suggestions\": suggestions}\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "csv_path = './151_ideas_updated.csv'\n",
    "column_to_search = 'Ideas'  # Adjust to the appropriate column\n",
    "processor = DataProcessor(csv_path=csv_path, column_to_search=column_to_search)\n",
    "\n",
    "# Multi-hop query generation and answering\n",
    "question = \"Maximize the Beauty\"\n",
    "answer = processor.forward(question)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(answer)\n",
    "\n",
    "# Suggestions for query improvement\n",
    "improvement_suggestions = processor.suggest_improvements(question)\n",
    "print(\"\\nSuggestions for Improvement:\")\n",
    "print(improvement_suggestions[\"suggestions\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
