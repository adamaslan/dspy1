{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df51f88add1a43d399b9c0049097f407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372e016230974036b8d7a767a0753f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "It means to maximize the beauty of your body. The beauty of the body is the power you have. It's possible to do things that will make you look better, but it's also possible to do things that will make you look worse. It's not about a physical thing, it's about making people look better.\n",
      "Question: how does one maximize the beauty of the body?\n"
     ]
    }
   ],
   "source": [
    "# works but response lame\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    A wrapper around the Hugging Face transformers pipeline for text generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\", **kwargs):\n",
    "        self.generator = pipeline(\"text-generation\", model=model_name, **kwargs)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        # Filter out any unsupported kwargs for the generate method\n",
    "        valid_kwargs = {key: kwargs[key] for key in [\"max_length\", \"num_return_sequences\", \"do_sample\", \"temperature\"] if key in kwargs}\n",
    "        \n",
    "        # Generate the response\n",
    "        response = self.generator(\n",
    "            prompt,\n",
    "            max_length=valid_kwargs.get(\"max_length\", 100),\n",
    "            num_return_sequences=valid_kwargs.get(\"num_return_sequences\", 1),\n",
    "            do_sample=valid_kwargs.get(\"do_sample\", True),\n",
    "            temperature=valid_kwargs.get(\"temperature\", 0.7)\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # Clean up the generated text\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "        return response\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str):\n",
    "        self.df = pd.read_csv(csv_path, quotechar='\"', escapechar='\\\\', skipinitialspace=True, on_bad_lines='skip')\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "    def safe_contains(self, text, query):\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        return query.lower() in text.lower()\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        results = self.df[\n",
    "            self.df[self.column_to_search].apply(\n",
    "                lambda x: self.safe_contains(str(x), query)\n",
    "            )\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def preprocess(self, question):\n",
    "        # Example preprocessing: you can customize this as needed\n",
    "        return question.lower()\n",
    "\n",
    "    def postprocess(self, context):\n",
    "        # Example postprocessing: you can customize this as needed\n",
    "        return context\n",
    "\n",
    "class RAG:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation class to combine context retrieval and generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, lm, csv_path: str, column_to_search: str):\n",
    "        self.lm = lm\n",
    "        self.processor = DataProcessor(csv_path, column_to_search)\n",
    "\n",
    "    def respond(self, context, question):\n",
    "        # Combine context and question to form a prompt\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "        return self.lm(prompt)\n",
    "\n",
    "    def forward(self, question):\n",
    "        # Preprocess the question\n",
    "        question = self.processor.preprocess(question)\n",
    "\n",
    "        # Retrieve relevant context\n",
    "        context_df = self.processor.retrieve(question)\n",
    "        context = context_df.head(5).to_string(index=False) if not context_df.empty else \"No relevant context found.\"\n",
    "\n",
    "        # Generate a response using the LM\n",
    "        return self.respond(context=context, question=question)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    lm = HuggingFaceLanguageModel(model_name=\"gpt2\")\n",
    "    csv_path = \"./151_ideas_updated.csv\"  # Replace with your actual CSV path\n",
    "    column_to_search = \"Ideas\"  # Replace with your column name\n",
    "\n",
    "    rag = RAG(lm, csv_path, column_to_search)\n",
    "\n",
    "    question = \"What does it mean to Maximize the Beauty?\"\n",
    "    print(\"Answer:\")\n",
    "    print(rag.forward(question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\t*** In DSPy 2.5, all LM clients except `dspy.LM` are deprecated, underperform, and are about to be deleted. ***\n",
      " \t\tYou are using the client HuggingFaceLanguageModel, which will be removed in DSPy 2.6.\n",
      " \t\tChanging the client is straightforward and will let you use new features (Adapters) that improve the consistency of LM outputs, especially when using chat LMs. \n",
      "\n",
      " \t\tLearn more about the changes and how to migrate at\n",
      " \t\thttps://github.com/stanfordnlp/dspy/blob/main/examples/migration.ipynb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['lm'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 158\u001b[0m\n\u001b[1;32m    156\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat does it mean to Maximize the Beauty?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSuggestions for Query Improvement:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m processor \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39mprocessor\n",
      "Cell \u001b[0;32mIn[14], line 146\u001b[0m, in \u001b[0;36mRAG.forward\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    143\u001b[0m     context \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo relevant context found.\u001b[39m\u001b[38;5;124m'\u001b[39m}]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Generate a response using the LM\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/primitives/program.py:24\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/chain_of_thought.py:44\u001b[0m, in \u001b[0;36mChainOfThought.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivated \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m     43\u001b[0m signature \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_signature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict\u001b[38;5;241m.\u001b[39mextended_signature \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature)\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/utils/callback.py:202\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:154\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:203\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         completions \u001b[38;5;241m=\u001b[39m new_generate(lm, signature, dsp\u001b[38;5;241m.\u001b[39mExample(demos\u001b[38;5;241m=\u001b[39mdemos, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m         completions \u001b[38;5;241m=\u001b[39m \u001b[43mold_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m pred \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mfrom_completions(completions, signature\u001b[38;5;241m=\u001b[39msignature)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mtrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dspy/predict/predict.py:229\u001b[0m, in \u001b[0;36mold_generate\u001b[0;34m(demos, signature, kwargs, config, lm, stage)\u001b[0m\n\u001b[1;32m    226\u001b[0m template \u001b[38;5;241m=\u001b[39m signature_to_template(signature)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# Note: query_only=True means the instructions and examples are not included.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/dsp/primitives/predict.py:73\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 73\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m, in \u001b[0;36mHuggingFaceLanguageModel.__call__\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Merge default kwargs with any overrides\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     effective_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shorter max length\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_kwargs\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Clean up the generated text\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:1309\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/generation/utils.py:1972\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1969\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1972\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/generation/utils.py:1360\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1360\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1363\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['lm'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "# chain of thought added\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import dspy\n",
    "\n",
    "\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    Custom Language Model wrapper for Hugging Face transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilgpt2', **kwargs):\n",
    "        # Ensure default temperature and other parameters\n",
    "        self.kwargs = {\n",
    "            \"temperature\": 0.7,  # Default temperature\n",
    "            **kwargs\n",
    "        }\n",
    "        self.generator = pipeline(\n",
    "            'text-generation', \n",
    "            model=model_name, \n",
    "            device=0,  # Use GPU if available\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        # Merge default kwargs with any overrides\n",
    "        effective_kwargs = {**self.kwargs, **kwargs}\n",
    "        response = self.generator(\n",
    "            prompt, \n",
    "            max_length=100,  # Shorter max length\n",
    "            num_return_sequences=1,\n",
    "            **effective_kwargs\n",
    "        )[0]['generated_text']\n",
    "\n",
    "        # Clean up the generated text\n",
    "        response = response.replace(prompt, '').strip()\n",
    "        return response\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        self.df = pd.read_csv(csv_path, \n",
    "                              quotechar='\"', \n",
    "                              escapechar='\\\\', \n",
    "                              skipinitialspace=True,\n",
    "                              on_bad_lines='skip')\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "        self.language_model = HuggingFaceLanguageModel(model_name=model_name)\n",
    "\n",
    "    def safe_contains(self, text, query):\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        return query.lower() in text.lower()\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        results = self.df[\n",
    "            self.df[self.column_to_search].apply(\n",
    "                lambda x: self.safe_contains(str(x), query)\n",
    "            )\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def generate_query(self, context: list, question: str):\n",
    "        context_str = str(context) if context else \"No previous context\"\n",
    "        prompt = (\n",
    "            f\"Context: {context_str}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Generate a concise keyword or phrase to search for relevant information.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def generate_answer(self, context: pd.DataFrame, question: str):\n",
    "        context_text = context.head(5).to_string(index=False)\n",
    "        prompt = (\n",
    "            f\"Context: {context_text}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Provide a clear and concise answer based on the context.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        context, queries = [], [question]\n",
    "\n",
    "        for hop in range(2):\n",
    "            query = self.generate_query(context=context, question=question)\n",
    "            query = query[:50]\n",
    "            if query.lower() in [q.lower() for q in queries]:\n",
    "                query = question.split()[:2]\n",
    "\n",
    "            print(f\"Generated Query: {query}\")\n",
    "\n",
    "            hop_results = self.retrieve(query)\n",
    "            if not hop_results.empty:\n",
    "                context.append(hop_results)\n",
    "            queries.append(query)\n",
    "\n",
    "        context_df = pd.concat(context).drop_duplicates() if context else self.df\n",
    "        return self.generate_answer(context=context_df, question=question)\n",
    "\n",
    "    def suggest_improvements(self, query: str):\n",
    "        context = self.retrieve(query)\n",
    "\n",
    "        if context.empty:\n",
    "            return {\"query\": query, \"suggestions\": \"No results found.\"}\n",
    "\n",
    "        prompt = (\n",
    "            f\"Context: {context.to_string(index=False)}\\n\"\n",
    "            f\"Past Query: {query}\\n\"\n",
    "            \"Suggest refinements to improve the search query.\"\n",
    "        )\n",
    "        suggestions = self.language_model(prompt)\n",
    "        return {\"query\": query, \"suggestions\": suggestions}\n",
    "\n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation module.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        self.processor = DataProcessor(csv_path, column_to_search, model_name)\n",
    "\n",
    "        # Define a custom Hugging Face LM and set it in dspy settings\n",
    "        self.lm = HuggingFaceLanguageModel(model_name=model_name)\n",
    "        dspy.settings.lm = self.lm  # Set the LM globally for dspy\n",
    "\n",
    "        # Initialize the chain of thought with the LM\n",
    "        self.respond = dspy.ChainOfThought('context, question -> response', lm=self.lm)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        # Retrieve relevant context\n",
    "        context = self.processor.retrieve(question).head(5).to_dict('records')\n",
    "        if not context:\n",
    "            context = [{'message': 'No relevant context found.'}]\n",
    "        \n",
    "        # Generate a response using the LM\n",
    "        return self.respond(context=context, question=question)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = './151_ideas_updated.csv'\n",
    "    column_to_search = 'Ideas'  # Adjust to the appropriate column\n",
    "\n",
    "    rag = RAG(csv_path=csv_path, column_to_search=column_to_search)\n",
    "\n",
    "    question = \"What does it mean to Maximize the Beauty?\"\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(rag.forward(question))\n",
    "\n",
    "    print(\"\\nSuggestions for Query Improvement:\")\n",
    "    processor = rag.processor\n",
    "    improvement_suggestions = processor.suggest_improvements(question)\n",
    "    print(improvement_suggestions[\"suggestions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Query: As you can see, the following keywords are added t\n",
      "Generated Query: This is usually done through keyword search, but a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      "\n",
      "\n",
      "Suggestions for Improvement:\n",
      "No results found, so suggestions cannot be provided.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    Custom Language Model wrapper for Hugging Face transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilgpt2'):  # Use a lightweight model for efficiency\n",
    "        self.generator = pipeline('text-generation', \n",
    "                                  model=model_name, \n",
    "                                  device=0,  # Use GPU if available\n",
    "                                  truncation=True)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        response = self.generator(\n",
    "            prompt, \n",
    "            max_length=len(prompt) + 250,   # Shorter max length\n",
    "            num_return_sequences=1,\n",
    "            **kwargs\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        # Clean up the generated text\n",
    "        response = response.replace(prompt, '').strip()\n",
    "        return response\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        # Load CSV data with more robust parsing\n",
    "        self.df = pd.read_csv(csv_path, \n",
    "                               quotechar='\"', \n",
    "                               escapechar='\\\\', \n",
    "                               skipinitialspace=True,\n",
    "                               on_bad_lines='skip')\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        # Verify the column exists\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "        # Initialize the language model\n",
    "        self.language_model = HuggingFaceLanguageModel(model_name=model_name)\n",
    "\n",
    "    def generate_query(self, context: list, question: str):\n",
    "        \"\"\"\n",
    "        Generate a query using context and a question.\n",
    "        \"\"\"\n",
    "        # Specific prompt to generate a focused query\n",
    "        context_str = str(context) if context else \"No previous context\"\n",
    "        prompt = (\n",
    "            f\"Context: {context_str}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Generate a very short, specific keyword or phrase to search for relevant information. \"\n",
    "            \"Focus on the key concept directly related to the question.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def safe_contains(self, text, query):\n",
    "        \"\"\"\n",
    "        Safely check if query is in text, using simple string matching\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return False\n",
    "        \n",
    "        # Use simple case-insensitive substring search\n",
    "        return query.lower() in text.lower()\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieve data from the CSV based on the generated query.\n",
    "        \"\"\"\n",
    "        # Use a custom search function to avoid regex parsing issues\n",
    "        results = self.df[\n",
    "            self.df[self.column_to_search].apply(\n",
    "                lambda x: self.safe_contains(str(x), query)\n",
    "            )\n",
    "        ]\n",
    "        return results\n",
    "\n",
    "    def generate_answer(self, context: pd.DataFrame, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer based on retrieved context.\n",
    "        \"\"\"\n",
    "        # Truncate context if it's too large\n",
    "        context_text = context.head(5).to_string(index=False)\n",
    "        \n",
    "        prompt = (\n",
    "            f\"Context: {context_text}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Based on the given context, provide a clear and concise answer. \"\n",
    "            \"Focus on explaining the key concept directly related to the question.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        \"\"\"\n",
    "        Multi-hop process for query generation, retrieval, and summarization.\n",
    "        \"\"\"\n",
    "        context, queries = [], [question]\n",
    "\n",
    "        for hop in range(2):\n",
    "            # Generate a query based on existing context\n",
    "            query = self.generate_query(context=context, question=question)\n",
    "            \n",
    "            # Ensure query is not empty or too similar\n",
    "            query = query[:50]  # Truncate to reasonable length\n",
    "            if query.lower() in [q.lower() for q in queries]:\n",
    "                query = question.split()[:2]  # Fallback to first two words\n",
    "            \n",
    "            print(f\"Generated Query: {query}\")\n",
    "            \n",
    "            # Retrieve results and add to context\n",
    "            hop_results = self.retrieve(query)\n",
    "            if not hop_results.empty:\n",
    "                context.append(hop_results)\n",
    "            queries.append(query)\n",
    "\n",
    "        # Generate final answer from collected context\n",
    "        context_df = pd.concat(context).drop_duplicates() if context else self.df\n",
    "        return self.generate_answer(context=context_df, question=question)\n",
    "\n",
    "    def suggest_improvements(self, query: str):\n",
    "        \"\"\"\n",
    "        Suggest improvements to refine the search query.\n",
    "        \"\"\"\n",
    "        # Perform a search and collect context\n",
    "        context = self.retrieve(query)\n",
    "\n",
    "        if context.empty:\n",
    "            return {\"query\": query, \"suggestions\": \"No results found, so suggestions cannot be provided.\"}\n",
    "\n",
    "        # Generate improvement suggestions\n",
    "        prompt = (\n",
    "            f\"Context: {context.to_string(index=False)}\\n\"\n",
    "            f\"Past Query: {query}\\n\"\n",
    "            \"Instruction: Suggest ways to improve or refine the search query for better results.\"\n",
    "        )\n",
    "        suggestions = self.language_model(prompt)\n",
    "        return {\"query\": query, \"suggestions\": suggestions}\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "csv_path = './151_ideas_updated.csv'\n",
    "column_to_search = 'Ideas'  # Adjust to the appropriate column\n",
    "processor = DataProcessor(csv_path=csv_path, column_to_search=column_to_search)\n",
    "\n",
    "# Multi-hop query generation and answering\n",
    "question = \"what is it to Maximize the Beauty?\"\n",
    "answer = processor.forward(question)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(answer)\n",
    "\n",
    "# Suggestions for query improvement\n",
    "improvement_suggestions = processor.suggest_improvements(question)\n",
    "print(\"\\nSuggestions for Improvement:\")\n",
    "print(improvement_suggestions[\"suggestions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 110, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./151_ideas_updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    116\u001b[0m column_to_search \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdeas\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Adjust to the appropriate column\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mDataProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_to_search\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_to_search\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Multi-hop query generation and answering\u001b[39;00m\n\u001b[1;32m    120\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximize the Beauty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mDataProcessor.__init__\u001b[0;34m(self, csv_path, column_to_search, model_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path: \u001b[38;5;28mstr\u001b[39m, column_to_search: \u001b[38;5;28mstr\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Load CSV data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_to_search \u001b[38;5;241m=\u001b[39m column_to_search\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Verify the column exists\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 6 fields in line 110, saw 9\n"
     ]
    }
   ],
   "source": [
    "# from docs rag 2\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class HuggingFaceLanguageModel:\n",
    "    \"\"\"\n",
    "    Custom Language Model wrapper for Hugging Face transformers\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilgpt2'):  # Use a lightweight model for efficiency\n",
    "        self.generator = pipeline('text-generation', model=model_name)\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        response = self.generator(prompt, max_length=150, **kwargs)[0]['generated_text']\n",
    "        return response\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class to process CSV data, search, summarize, and analyze insights\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: str, column_to_search: str, model_name='distilgpt2'):\n",
    "        # Load CSV data\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.column_to_search = column_to_search\n",
    "\n",
    "        # Verify the column exists\n",
    "        if self.column_to_search not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column_to_search}' not found in the CSV.\")\n",
    "\n",
    "        # Initialize the language model\n",
    "        self.language_model = HuggingFaceLanguageModel(model_name=model_name)\n",
    "\n",
    "    def generate_query(self, context: list, question: str):\n",
    "        \"\"\"\n",
    "        Generate a query using context and a question.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Context: {context}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Generate a distinct and concise query to retrieve relevant data.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        \"\"\"\n",
    "        Retrieve data from the CSV based on the generated query.\n",
    "        \"\"\"\n",
    "        results = self.df[self.df[self.column_to_search].str.contains(query, case=False, na=False)]\n",
    "        return results\n",
    "\n",
    "    def generate_answer(self, context: pd.DataFrame, question: str):\n",
    "        \"\"\"\n",
    "        Generate an answer based on retrieved context.\n",
    "        \"\"\"\n",
    "        context_text = context.to_string(index=False)\n",
    "        prompt = (\n",
    "            f\"Context: {context_text}\\n\"\n",
    "            f\"Question: {question}\\n\"\n",
    "            \"Provide a detailed summary of the retrieved data.\"\n",
    "        )\n",
    "        return self.language_model(prompt)\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        \"\"\"\n",
    "        Multi-hop process for query generation, retrieval, and summarization.\n",
    "        \"\"\"\n",
    "        context, queries = [], [question]\n",
    "\n",
    "        for hop in range(2):\n",
    "            query = self.generate_query(context=context, question=question)\n",
    "            \n",
    "            # Assertions and suggestions\n",
    "            if len(query) >= 100:\n",
    "                print(\"FAIL!  Query too long. Regenerating with updated prompt...\")\n",
    "                query = self.generate_query(context=context, question=f\"{question} (Keep it concise)\")\n",
    "\n",
    "            if query in queries:\n",
    "                print(f\"FAIL!  Query not distinct from previous attempts. Regenerating...\")\n",
    "                query = self.generate_query(\n",
    "                    context=context,\n",
    "                    question=f\"{question} (Avoid similarity with previous attempts: {queries})\"\n",
    "                )\n",
    "\n",
    "            print(f\"Generated Query: {query}\")\n",
    "            context.append(self.retrieve(query))\n",
    "            queries.append(query)\n",
    "\n",
    "        # Generate final answer from collected context\n",
    "        context_df = pd.concat(context).drop_duplicates()\n",
    "        return self.generate_answer(context=context_df, question=question)\n",
    "\n",
    "    def suggest_improvements(self, query: str):\n",
    "        \"\"\"\n",
    "        Suggest improvements to refine the search query.\n",
    "        \"\"\"\n",
    "        # Perform a search and collect context\n",
    "        context = self.retrieve(query)\n",
    "\n",
    "        if context.empty:\n",
    "            return {\"query\": query, \"suggestions\": \"No results found, so suggestions cannot be provided.\"}\n",
    "\n",
    "        # Generate improvement suggestions\n",
    "        prompt = (\n",
    "            f\"Context: {context.to_string(index=False)}\\n\"\n",
    "            f\"Past Query: {query}\\n\"\n",
    "            \"Instruction: Suggest ways to improve or refine the search query for better results.\"\n",
    "        )\n",
    "        suggestions = self.language_model(prompt)\n",
    "        return {\"query\": query, \"suggestions\": suggestions}\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "csv_path = './151_ideas_updated.csv'\n",
    "column_to_search = 'Ideas'  # Adjust to the appropriate column\n",
    "processor = DataProcessor(csv_path=csv_path, column_to_search=column_to_search)\n",
    "\n",
    "# Multi-hop query generation and answering\n",
    "question = \"Maximize the Beauty\"\n",
    "answer = processor.forward(question)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(answer)\n",
    "\n",
    "# Suggestions for query improvement\n",
    "improvement_suggestions = processor.suggest_improvements(question)\n",
    "print(\"\\nSuggestions for Improvement:\")\n",
    "print(improvement_suggestions[\"suggestions\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
